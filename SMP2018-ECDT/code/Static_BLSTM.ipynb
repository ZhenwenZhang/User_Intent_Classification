{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:26:26.368360Z",
     "start_time": "2019-01-16T12:26:24.911651Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jieba as jb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "gpuconfig = tf.ConfigProto(log_device_placement=True)\n",
    "gpuconfig.gpu_options.allow_growth = True\n",
    "\n",
    "estimator_config = tf.estimator.RunConfig(\n",
    "    model_dir='/home/zhangzw/logs/SMP2018/Static-Bi-LSTM',\n",
    "    save_summary_steps=100,\n",
    "    session_config=gpuconfig)\n",
    "\n",
    "base_dir = '/home/zhangzw/SMP2018-ECDT/'\n",
    "category_file = os.path.join(base_dir, 'data/category.txt')\n",
    "train_file = os.path.join(base_dir, 'data/train.txt')\n",
    "develop_file = os.path.join(base_dir, 'data/develop.txt')\n",
    "test_file = os.path.join(base_dir, 'data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:26:28.290924Z",
     "start_time": "2019-01-16T12:26:26.371026Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.688 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Read data from file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Building vocabulary from train set.\n",
      "INFO:Compute the maximum length of sentence.\n",
      "INFO:Convert words to id represent.\n",
      "INFO:Padding sentences.\n",
      "INFO:Maximum  length of sentence is 26.\n",
      "INFO:Vocabulary size is 2889.\n",
      "INFO:train set shape is (2299, 26).\n",
      "INFO:develop set shape is (770, 26).\n",
      "INFO:test set shape is (666, 26).\n"
     ]
    }
   ],
   "source": [
    "class SMPDATASET(object):\n",
    "    def __init__(self, filenames, category_file):\n",
    "        self.__filenames = filenames\n",
    "        self.__category_file = category_file\n",
    "        self.vocab_size = 0\n",
    "        self.seq_max_len = 0\n",
    "        self.word_index = {}\n",
    "\n",
    "    def __read_category(self):\n",
    "        category_table = {}\n",
    "        for line in open(self.__category_file).readlines():\n",
    "            category = line.strip().split(':')[0]\n",
    "            label = line.strip().split(':')[1]\n",
    "            category_table[category] = label\n",
    "        return category_table\n",
    "\n",
    "    def __read_data(self):\n",
    "        dataset = {}\n",
    "        labels = ['train', 'develop', 'test']\n",
    "        for filename, label in zip(self.__filenames, labels):\n",
    "            data = [\n",
    "                line.strip().split(\"\\t\")\n",
    "                for line in open(filename).readlines()\n",
    "            ]\n",
    "            y = [self.__read_category()[item[0]] for item in data]\n",
    "            x = [list(jb.cut(item[1])) for item in data]\n",
    "            dataset[label] = (x, np.array(y, dtype=int))\n",
    "        return dataset\n",
    "\n",
    "    def __build_vocab(self, sentences):\n",
    "        vocab_size = 0\n",
    "        word_index = {}\n",
    "        all_words = set([word for item in sentences for word in item])\n",
    "        word_index[\"<PAD>\"] = 0\n",
    "        word_index[\"<UNK>\"] = 1\n",
    "\n",
    "        for index, word in enumerate(all_words):\n",
    "            word_index[word] = index + 2\n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        return vocab_size, word_index\n",
    "\n",
    "    def __get_max_len(self, sentences):\n",
    "        return max([len(sentence) for sentence in sentences])\n",
    "\n",
    "    def __word2index(self, sentences):\n",
    "        results = []\n",
    "        for sentence in sentences:\n",
    "            sentence_id = []\n",
    "            for word in sentence:\n",
    "                if word in self.word_index.keys():\n",
    "                    sentence_id.append(self.word_index[word])\n",
    "                else:\n",
    "                    sentence_id.append(self.word_index[\"<UNK>\"])\n",
    "            results.append(sentence_id)\n",
    "        return results\n",
    "\n",
    "    def __padding_sentence(self, sentences):\n",
    "        padding_result = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sentences,\n",
    "            maxlen=self.seq_max_len,\n",
    "            truncating='post',\n",
    "            padding='post',\n",
    "            value=self.word_index['<PAD>'])\n",
    "        return padding_result\n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"INFO:Read data from file.\")\n",
    "        x_train, y_train = self.__read_data()['train']\n",
    "        x_develop, y_develop = self.__read_data()['develop']\n",
    "        x_test, y_test = self.__read_data()['test']\n",
    "\n",
    "        print(\"INFO:Building vocabulary from train set.\")\n",
    "        self.vocab_size, self.word_index = self.__build_vocab(x_train)\n",
    "\n",
    "        print(\"INFO:Compute the maximum length of sentence.\")\n",
    "        self.seq_max_len = self.__get_max_len(x_train)\n",
    "\n",
    "        print(\"INFO:Convert words to id represent.\")\n",
    "        x_train_id = self.__word2index(x_train)\n",
    "        x_develop_id = self.__word2index(x_develop)\n",
    "        x_test_id = self.__word2index(x_test)\n",
    "        x_len_train = np.array(\n",
    "            [min(len(x), self.seq_max_len) for x in x_train_id])\n",
    "        x_len_develop = np.array(\n",
    "            [min(len(x), self.seq_max_len) for x in x_develop_id])\n",
    "        x_len_test = np.array(\n",
    "            [min(len(x), self.seq_max_len) for x in x_test_id])\n",
    "\n",
    "        print(\"INFO:Padding sentences.\")\n",
    "        x_train_pad = self.__padding_sentence(x_train_id)\n",
    "        x_develop_pad = self.__padding_sentence(x_develop_id)\n",
    "        x_test_pad = self.__padding_sentence(x_test_id)\n",
    "\n",
    "        print(\"INFO:Maximum  length of sentence is {}.\".format(\n",
    "            self.seq_max_len))\n",
    "        print(\"INFO:Vocabulary size is {}.\".format(self.vocab_size))\n",
    "        print(\"INFO:train set shape is {}.\".format(x_train_pad.shape))\n",
    "        print(\"INFO:develop set shape is {}.\".format(x_develop_pad.shape))\n",
    "        print(\"INFO:test set shape is {}.\".format(x_test_pad.shape))\n",
    "\n",
    "        return (x_train_pad, x_len_train,\n",
    "                y_train), (x_develop_pad, x_len_develop,\n",
    "                           y_develop), (x_test_pad, x_len_test, y_test)\n",
    "\n",
    "\n",
    "filenames = [train_file, develop_file, test_file]\n",
    "(x_train, x_len_train,\n",
    " y_train), (x_develop, x_len_develop,\n",
    "            y_develop), (x_test, x_len_test, y_test) = SMPDATASET(\n",
    "                filenames, category_file).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:26:28.299202Z",
     "start_time": "2019-01-16T12:26:28.294770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0),\n",
    "    'hidden_size': 128,\n",
    "    'num_classes': 31,\n",
    "    'embedding_size': 128,\n",
    "    'vocab_size': 2889,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:26:28.391456Z",
     "start_time": "2019-01-16T12:26:28.300724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train,\n",
    "                                                  y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x_train))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_develop, x_len_develop,\n",
    "                                                  y_develop))\n",
    "    dataset = dataset.batch(len(x_develop))\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "def test_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(len(x_test))\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:26:28.479334Z",
     "start_time": "2019-01-16T12:26:28.393284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/zhangzw/logs/SMP2018/Static-Bi-LSTM', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "log_device_placement: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc0f67422b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'],\n",
    "        params['vocab_size'],\n",
    "        params['embedding_size'],\n",
    "        initializer=params['embedding_initializer'])\n",
    "\n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=params['hidden_size'])\n",
    "    lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=params['hidden_size'])\n",
    "    outputs, output_state_fw, output_state_bw = tf.nn.static_bidirectional_rnn(\n",
    "        cell_fw=lstm_cell_bw,\n",
    "        cell_bw=lstm_cell_fw,\n",
    "        inputs=tf.unstack(input_layer, 26, 1),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "    concat_result = tf.concat([output_state_fw.h, output_state_bw.h],\n",
    "                              axis=-1,\n",
    "                              name='Concat')\n",
    "\n",
    "    dropout_hidden = tf.layers.dropout(\n",
    "        inputs=concat_result, rate=0.5, training=training, name='Dropout')\n",
    "\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=dropout_hidden, units=params['num_classes'], name='Fc')\n",
    "\n",
    "    predicted_labels = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'pre_labels': predicted_labels[:, tf.newaxis]}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=labels, predictions=predicted_labels, name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=params['learning_rate'], name='Adam')\n",
    "        train_op = optimizer.minimize(\n",
    "            loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "model = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, config=estimator_config, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:27:59.674803Z",
     "start_time": "2019-01-16T12:26:28.480986Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/zhangzw/logs/SMP2018/Static-Bi-LSTM/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.5197206, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.7977\n",
      "INFO:tensorflow:loss = 2.314978, step = 100 (4.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6308\n",
      "INFO:tensorflow:loss = 1.2466422, step = 200 (2.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.4679\n",
      "INFO:tensorflow:loss = 0.44659108, step = 300 (2.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1617\n",
      "INFO:tensorflow:loss = 0.37652177, step = 400 (2.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6295\n",
      "INFO:tensorflow:loss = 0.31662953, step = 500 (2.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4943\n",
      "INFO:tensorflow:loss = 0.14758022, step = 600 (2.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0122\n",
      "INFO:tensorflow:loss = 0.25301647, step = 700 (2.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3526\n",
      "INFO:tensorflow:loss = 0.05317331, step = 800 (2.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.896\n",
      "INFO:tensorflow:loss = 0.05324799, step = 900 (2.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.9483\n",
      "INFO:tensorflow:loss = 0.061700393, step = 1000 (2.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2509\n",
      "INFO:tensorflow:loss = 0.07458775, step = 1100 (2.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6259\n",
      "INFO:tensorflow:loss = 0.08733175, step = 1200 (2.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2253\n",
      "INFO:tensorflow:loss = 0.045757707, step = 1300 (2.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.858\n",
      "INFO:tensorflow:loss = 0.02470259, step = 1400 (2.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5651\n",
      "INFO:tensorflow:loss = 0.029286, step = 1500 (2.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2468\n",
      "INFO:tensorflow:loss = 0.0043423213, step = 1600 (2.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0062\n",
      "INFO:tensorflow:loss = 0.010461958, step = 1700 (2.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2447\n",
      "INFO:tensorflow:loss = 0.025450245, step = 1800 (2.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2897\n",
      "INFO:tensorflow:loss = 0.019131288, step = 1900 (2.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6191\n",
      "INFO:tensorflow:loss = 0.008484411, step = 2000 (2.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.545\n",
      "INFO:tensorflow:loss = 0.0068806447, step = 2100 (2.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.9011\n",
      "INFO:tensorflow:loss = 0.002423457, step = 2200 (2.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7958\n",
      "INFO:tensorflow:loss = 0.017079258, step = 2300 (2.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6759\n",
      "INFO:tensorflow:loss = 0.0015598612, step = 2400 (2.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.651\n",
      "INFO:tensorflow:loss = 0.0037928638, step = 2500 (2.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5702\n",
      "INFO:tensorflow:loss = 0.006962139, step = 2600 (2.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0502\n",
      "INFO:tensorflow:loss = 0.002130906, step = 2700 (2.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.3808\n",
      "INFO:tensorflow:loss = 0.0051385667, step = 2800 (2.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.5385\n",
      "INFO:tensorflow:loss = 0.0028436256, step = 2900 (2.527 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /home/zhangzw/logs/SMP2018/Static-Bi-LSTM/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0012338609.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fc07c3c5908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:28:01.422509Z",
     "start_time": "2019-01-16T12:27:59.676946Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-16-12:28:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/zhangzw/logs/SMP2018/Static-Bi-LSTM/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-16-12:28:00\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.81038964, global_step = 3000, loss = 1.0596498\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3000: /home/zhangzw/logs/SMP2018/Static-Bi-LSTM/model.ckpt-3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.81038964, 'loss': 1.0596498, 'global_step': 3000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:28:01.428884Z",
     "start_time": "2019-01-16T12:28:01.426189Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pre_results = np.array([\n",
    "#     pre_label['pre_labels'][0]\n",
    "#     for pre_label in model.predict(input_fn=test_input_fn)\n",
    "# ])\n",
    "# print(\"\\nClassification Report:\\n\",\n",
    "#       metrics.classification_report(y_test, pre_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZhenWen-Py3",
   "language": "python",
   "name": "zhenwen_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
